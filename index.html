<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xinzhou Wang</title>
  
  <meta name="author" content="Xinzhou Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">

</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:left">
                <name>Xinzhou Wang | 王心舟</name>
              </p>
              <br>
              <p style="line-height: 1.6"> 
                I am a third-year PhD student at  <a href="https://see.tongji.edu.cn/">College of Electronic Information and Engineering, Tongji University</a> and <a href="https://www.cs.tsinghua.edu.cn/">Department of Computer Science, Tsinghua University</a>, 
                  advised by <a href="https://www.cs.tsinghua.edu.cn/csen/info/1154/3972.htm">Prof.Fuchun Sun</a>. 
                  I obtained my B.S. and M.S at Beihang University.
              </p>
              <p style="line-height: 1.6">
                My research interest lies in the <b>3D/4D Generation</b>,  <b>Video Generation</b>, and <b>3D Reconstruction</b>. I am particularly interested in combining video diffusion model with sequential 3D generation methods. 
              </p>
              <p style="text-align:left">
                <a href="mailto:wangxinzhou.buaa@foxmail.com">Email</a> &nbsp/&nbsp
                <a href="https://zz7379.github.io/">CV</a> &nbsp/&nbsp
                <a href="https://zz7379.github.io/"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/zz7379"> Github </a>
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:70%;max-width:70%" alt="profile photo" src="images/wxz2.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <p><heading>News</heading></p>
          </td>
        </tr>
      </tbody></table>
          <li style="margin: 20px;">
              <b>2024-11:</b><a href="https://github.com/Tencent/Hunyuan3D-1">Tencent Hunyuan3D-1.0</a></a>.
              </li>
          <li style="margin: 20px;">
              <b>2024-10:</b> Vidu4D is accepted by NeurIPS 2024</a>.
              </li>
          <li style="margin: 20px;">
              <b>2024-07:</b> Two papers on 3D AIGC are accepted by <a href="https://eccv.ecva.net/">ECCV 2024</a>.
              </li>
          <li style="margin: 20px;">
            <b>2024-05:</b> New work: Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels.
            </li>
          <li style="margin: 20px;">
            <b>2024-03:</b> New work: accepted by IEEE Transactions on Cybernetics: Digital-Twin-Assisted Skill Learning for 3C Assembly Tasks.
            </li>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/hunyuan3d.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation</papertitle>
              <b>
		Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, <strong>Xinzhou Wang</strong>, Qingxiang Lin, Jiaao Yu, Lifu Wang, Zhuo Chen, Sicong Liu, Yuhong Liu, Yong Yang, Di Wang, Jie Jiang, Chunchao Guo
              <br>
              <em>Arxiv, 2024</em>
              <br>
              <a href="https://arxiv.org/pdf/2411.02293">[arXiv]</a>
              <a href="https://github.com/Tencent/Hunyuan3D-1">[Code]</a>
              <a href="https://3d.hunyuan.tencent.com/">[Project Page]</a> 
              <br>
              <p> We propose Tencent Hunyuan3D-1.0, a unified framework for text-to-3D and image-to-3D generation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pipeline2.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation 
                and Reconstruction with Canonical Score Distillation</papertitle>
              <br>
              <strong>Xinzhou Wang</strong>,
              Yikai Wang,
              Junliang Ye, 
              Zhengyi Wang,
              Fuchun Sun,
              Pengkun Liu,
              Ling Wang,
              Kai Sun, 
              Xintong Wang, 
              Bin He
              <br>
              <em>ECCV, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2312.03795">[arXiv]</a>
              <a href="https://github.com/AnimatableDreamer/AnimatableDreamer">[Code]</a>
              <a href="https://zz7379.github.io/AnimatableDreamer">[Project Page]</a> 
              <br>
              <p> We propose AnimatableDreamer, a framework with the capability to generate generic categories of non-rigid 3D models.</p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pipeline.jpg" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>DreamReward: Aligning Human Preference in Text-to-3D Generation</papertitle>
              <br>
              Junliang Ye*, 
              Fangfu Liu*, 
              Qixiu Li,
              Zhengyi Wang,
              Yikai Wang,
              <strong>Xinzhou Wang</strong>,
              Yueqi Duan </a>,
              Jun Zhu
              <br>
              <em>ECCV, 2024</em>
              <br>
              <a href="http://arxiv.org/abs/2403.14613">[arXiv]</a>
              <a href="https://github.com/liuff19/DreamReward">[Code]</a>
              <a href="https://jamesyjl.github.io/DreamReward/">[Project Page]</a> 
              <br>
              <p> We present a comprehensive framework, coined DreamReward, to 
                learn and improve text-to-3D models from human preference feedback.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Vidu4D.png" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels</papertitle>
              <br>
              Yikai Wang*, <strong>Xinzhou Wang*</strong>, Zilong Chen, Zhengyi Wang, Fuchun Sun, Jun Zhu

              <br>
              <em>NeurIPS, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2405.16822">[arXiv]</a>
              <a href="https://github.com/yikaiw/vidu4d">[Code]</a>
              <a href="https://vidu4d-dgs.github.io/">[Project Page]</a> 
              <a href="https://www.youtube.com/watch?v=4tUkDj3pglg/">[Youtube]</a> 
              <br>
              <p> We present a Vidu4D, a framework to reconstruct high-fidelity 4D model from generated videos.</p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Digital.png" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>Digital-Twin-Assisted Skill Learning for 3C Assembly Tasks</papertitle>
              <br>
              Fuchun Sun</strong>, Naijun Liu, <strong>Xinzhou Wang</strong>, Ruize Sun, Shengyi Miao, Zengxin Kang, Bin Fang, Huaping Liu, Yongjia Zhao, Haiming Huang
              
              <br>
              <em>IEEE Transactions on Cybernetics.</em>
              <br>
              <a href="https://ieeexplore.ieee.org/document/10492985/">[paper]</a>

              <br>
              <p> We present a digital twin with reinforce learning for sim-to-real 3C assembly task.</p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/3C.png" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>Sim-to-Real 3C Assembly with Physics-Integrated Implicit Digital Twin</papertitle>
              <br>
              <strong>Xinzhou Wang</strong>, Fuchun Sun, Ling Wang, Kai Sun, Yuanyan Xie, Xintong Wang, Bin He, Huaidong Zhou
              
              <br>
              <em></em>
              <br>
              <a href="https://arxiv.org">[arXiv]</a>

              <br>
              <p> We present a implict digital twin with physics engine for sim-to-real 3C assembly task.</p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Equivariant.png" alt="dise">
            </td>
            <td width="80%" valign="center">
              <papertitle>Equivariant Local Reference Frames for Unsupervised Non-rigid Point Cloud Shape Correspondence</papertitle>
              <br>
              Ling Wang, Runfa Chen, Yikai Wang, Fuchun Sun, <strong>Xinzhou Wang</strong>, Sun Kai, Guangyuan Fu, Jianwei Zhang, Wenbing Huang

              <br>
              <em>Submitted to IEEE Transactions on Image Processing, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2404.00959">[arXiv]</a>

              <br>
              <p> We present a framework for unsupervised non-rigid point cloud shape correspondence registration, incorporating principles of equivariance.</p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pipeline_iso.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding</papertitle>
              <br>
              Pengkun Liu, Yikai Wang, Fuchun Sun, Jiafang Li, Hang Xiao, Hongxiang Xue, <strong>Xinzhou Wang</strong>
              <br>
              <em>Arxiv, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2403.10395">[arXiv]</a>
              <a href="https://isotropic3d.github.io/">[Code]</a>
              <a href="https://github.com/pkunliu/Isotropic3D">[Project Page]</a> 
              <br>
              <p> We propose Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding as input.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/tongue.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Learning Based Tongue Prickles Detection in Traditional Chinese Medicine</papertitle>
              <br>
              <strong>Xinzhou Wang</strong>, Siyan Luo, Guihua Tian, Xiangrong Rao, Bin He, Fuchun Sun
              <br>
              <em>Evidence-Based Complementary and Alternative Medicine, 2022</em>
              <br>
              <a href="https://www.hindawi.com/journals/ecam/2022/5899975/">[Paper]</a>
              <br>
              <p> We provides a quantitative perspective for symptoms and disease diagnosis according to tongue characteristics.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:80%;max-width:80%" src="images/engine.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Measurement Selection Method for Aero-engine Discrete Operating Conditions Gas Path Analysis</papertitle>
              <br>
              Xintong Wang, <strong>Xinzhou Wang</strong>
              <br>
              <em>International Conference on Algorithms, Data Mining, and Information Technology (ADMIT), 2022</em>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9974728">[Paper]</a>
              <br>
              <p> We provides a method for measurement selection of Aero-engine gas path analysis.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:80%;max-width:80%" src="images/cos.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Camouflaged Object Segmentation with Transformer</papertitle>
              <br>
              Haiwen Wang*, <strong>Xinzhou Wang*</strong>, Fuchun Sun, Yixu Song
              <br>
              <em> Cognitive Systems and Information Processing, 2021</em>
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-981-16-9247-5_17">[Paper]</a>
              <br>
              <p> We provides a transformer for camouflaged object segmentation.</p>
            </td>
          </tr>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                Review for <b>CVPR 2023</b>.
              </li>
          </td>
        </tr> -->
      </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
      <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=0E8MTgY3uzPlMqQ6k4ja-Y8ajcyDF48Pd6VYZ5EfX_A"></script>
	  <br>
	    &copy; Xinzhou Wang | Last updated: 25 Mar, 2024
</center></p>
</body>

</html>
